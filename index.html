<!DOCTYPE html><!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Sun Nov 07 2021 09:43:13 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="6183c51c4606b5dd8c5fbe79" data-wf-site="6183c51c4606b562045fbe78">
<head>
  <meta charset="utf-8">
  <title>10000buddhas</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/10000buddhas.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Exo:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
</head>
<body class="body">
  <div class="headermain">
    <div class="headerimage">
      <div class="textbg">
        <h1 class="heading-3">10.000 Buddhas</h1>
        <div class="text-block-left">Principal Investigator: <br><b>Christian Sandor</b><br>Host Institution: <br><b>VENISE TEAM /<br>Paris-Saclay University</b><br><br>Submission for the:<br><b>Epic MegaGrants Scheme</b></div>
      </div>
    </div>
  </div>
  <div class="content">
  <div class="bodyheader-copy-main">Visionary Movie<br></div>
  <div class="container">
    <iframe class="responsive-iframe" src="https://www.youtube.com/embed/bZEzjVNvlNA" title="10,000 Buddhas - Visionary Movie" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>

  <!-- ABSTRACT -->

  <div class="bodyheader-copy">Abstract<br></div>

  <div class="noindent">10.000 Buddhas is a <b>digital art installation</b> using VR immersive environments. The artwork&#x27;s central elements are the statues at the <a href="http://www.hongkongextras.com/_ten_thousand_buddhas_monastery.html" target="_blank">Ten Thousand Buddhas Monastery</a> in Hong Kong [1]. Digital replicas of these statues populate the virtual environment. Using the <a href="http://digiscope.fr/en/platforms/eve" target="_blank">EVE (Evolutive Virtual Environment)</a> system [2], the users can immerse themselves in the monastery&#x27;s virtual reinterpretation.<br></div>

  <div class="bodytext">The EVE system is a <b>multi-user CAVE</b> allowing double stereoscopy. It incorporates high-quality hardware, such as an ambisonic sound system, high-precision tracking, haptic devices,and Barco projectors. This project will utilise Unreal Engine to control the listed technology and provide a holistic experience.<br></div>

  <img src="images/overview_converted.jpg" loading="lazy" sizes="80vw" srcset="images/overview_converted-p-500.jpeg 500w, images/overview_converted-p-800.jpeg 800w, images/overview_converted-p-1600.jpeg 1600w, images/overview_converted.jpg 1920w" alt="" class="bodyimagefull">

  <p class="figure">Figure 1. Overview of the 10.000 Buddhas artwork during operation in the EVE system. Due to the system&#x27;s double stereoscopy feature, it is possible to visualize different environments for two users.<br></p>

  <!-- DESCRIPTION -->

  <div class="bodyheader">Description<br></div>

  <div class="noindent">This section introduces the project's conceptual and technical framework. It describes how the project benefits from UE's vast features. Moreover, this section mentions how the Unreal Community can benefit from the project.<br><br></div>

  <!-- EVE SYSTEM -->

  <div class="bodyheader2">EVE System<br></div>

  <div class="noindent">The EVE system is a unique, multi-user CAVE with <b>multi-user 3D perception</b> capabilities. It is a construction of 3x 5m high HD projection screens, enclosing a 13 m<sup>2</sup> floor-glass display. It includes a motion-tracking apparatus, high-end 3D audio, and a large-scale haptic device.</div>

  <div class="bodytext">Currently, the EVE system runs on Unity. One of the project’s main goals is to shift the system’s core engine <b>from Unity to Unreal Engine</b>. There are many reasons why this change would be beneficial.</div>

  <div class="bodytext">For example, the UE’s visual scripting language has proven suitable for conducting tests and quick demonstrations of ideas. Visual scripting widens the range of potential intern candidatures, for example, design and art students. Imaginative visualisation of scientific problems will contribute to future research projects. Since 2021 Unity has included visual scripting features. In contrast, the UE introduced Blueprints in 2014, which indicates a more reliable system, vaster user base and more profound technical support.</div>

  <div class="bodytext">Unity cannot achieve such visually appealing global illumination rendering as UE does. Moreover, our proposal uses high-poly count objects, strengthening our reason for using UE over Unity. We believe that rendering quality and highly detailed virtual scenes will contribute to future projects conducted in our system. </div>

  <div class="bodytext">Our proposed artwork will be the first project to integrate UE into the EVE system. Besides Unity, our current setup uses other middleware tools, like MiddleVR, MAX/MSP or PureData. Having the possibility of developing projects for our system using only one powerful software will make our system more accessible for students.<br><br></div>

  <img src="images/eve_system.png" loading="lazy" sizes="80vw" srcset="images/eve_system-p-500.png 500w, images/eve_system-p-800.png 800w, images/eve_system-p-1080.png 1080w, images/eve_system-p-1600.png 1600w, images/eve_system.png 1920w" alt="" class="bodyimagefull">

  <p class="figure">Figure 2. Documentation of the EVE system in-situ. The system features four projection screens with the possibility of stereoscopic visualization, high-precision trackers, haptic devices, and an ambisonic sound system.<br></p>

  <!-- Unreal Engine -->

  <div class="bodyheader2">Unreal Engine</div>

  <div class="noindent">The project aims to use UE's existing features as extensively as possible in the EVE system. Our project will benefit from plenty of UE's features. However, many aspects of our proposal will require development on our side ("e.g., dual-stereoscopy, morphing of unrelated 3D meshes"). In this chapter, we introduce what we expect from UE. In the next chapter ("Contribution"), we list additional features initiated in our proposal.<br><br></div>

  <img src="images/2_photogrammetry.jpg" sizes="80vw" srcset="images/2_photogrammetry-p-500.jpeg 500w, images/2_photogrammetry-p-800.jpeg 800w, images/2_photogrammetry.jpg 1917w" alt="" class="bodyimagefull">

  <p class="figure">Figure 3. Digitalization of the Arhat statues. Picture a.) shows the real statue at the 10.000 Buddhas Monastery. Picture b.) shows the result of the photogrammetry process.<br></p>

  <div class="bodytext">The project aims to use UE's existing features as extensively as possible in the EVE system. Our project will benefit from plenty of UE's features. However, many aspects of our proposal will require development on our side ("e.g., dual-stereoscopy, morphing of unrelated 3D meshes"). In this chapter, we introduce what we expect from UE. In the next chapter ("Contribution"), we list additional features initiated in our proposal.<br><br></div>

  <div class="bodytext">We are using photogrammetry for constructing the 3D replicas of the <a href="https://www.britannica.com/topic/arhat">Arhat</a> [3] statues. We have tested many photogrammetry software and picked <b>RealityCapture</b> based on speed/ quality ratio. As we know, RealityCapture belongs to Epic, meaning that Epic's products cover our project's software requirements from preparation to development until implementation and performance.<br><br></div>

  <div class="bodytext">Let us further describe other features of Unreal Engine, which we plan to utilise in the EVE system. As we mentioned, the EVE system relies on an array of 7x HD Barco projectors. Additionally, these projectors allow stereoscopic visualisation. Moreover, the EVE system allows dual-stereoscopic visualisation, meaning that it is possible to immerse two people in different virtual environments simultaneously.</div>

  <div class="bodytext">To handle simultaneous, synchronised video outputs, we will use <b>nDisplay</b>. However, achieving dual-stereoscopic projection in the context of the EVE system (combining polarised and active stereo technologies) is not a straightforward task. We aim to develop a solution for dualstereoscopy within Unreal Engine.</div>

  <div class="bodytext">The EVE system relies on a cluster of computers for computing projection, ambisonic audio, tracking and haptic input. Solving such a multi-modal computing task will require several UE instances running simultaneously. For managing this network of UE instances, we will use the <b>Switchboard</b> application together with <b>Stage Monitor</b>. Switchboard will help us to distribute tasks using one master computer.</div>

  <div class="bodytext">The EVE system is a real-time immersive environment for conducting performance-based research. The main goal of our proposed project is to create a shared live experience. Running projects in a performative mode requires real-time monitoring of every performing computer and UE instance in the pipeline. Fortunately, UE's Stage Monitor application addresses our live monitoring requirements.<br><br></div>

  <div class="bodytext">Besides providing an immersive exposure for the users, our system tracks their position within its area. These positional values trigger interactive events during runtime. In this particular project, we use positional values and head tracking data, which will help us determine the users' direction of focus. 9x ART 3D motion sensors cover our system. ART motion sensors come with dedicated software, DTrack. UE's <b>LiveLink</b> feature will allow us to create a direct link between DTrack and UE. We plan to build upon an already existing solution published on GitHub [4].<br><br></div>

  <div class="bodytext">Ambisonic audio elevates the feeling of immersion in virtual environments. Therefore this technology plays a crucial part in our proposal. We plan to use audio in two ways. One way would be to utilise ambisonic technology, spreading various sound effects spatially around the users. The other usage would generate tactile stimuli for the users. Gestures of the users' will trigger the system's subwoofers to generate heavy bass and resonate the EVE environment's construction.</div>

  <div class="bodytext">We plan to use the introduced MetaSounds for composing the installation's sound design. Since MetaSounds also relies on Blueprints, it allows us to collaborate with sound engineers more easily. We are aware that <b>MetaSounds</b> cannot distribute sound onto ambisonic systems. Therefore, we will use existing plugins, SteamAudio [5].<br><br></div>

  <div class="bodytext">Lastly, the visual scripting feature (<b>Blueprints</b>) of UE will give us an advantage in creating interdisciplinary teams of engineers, artists and sound designers. We believe that the creative input of people from various backgrounds alleviates the quality of the projects. Moreover, allowing a vaster range of students to work with our system will definitely lead to a proliferation of compelling projects in the EVE environment.<br><br></div>

  <div class="bodytext">In conclusion, the EVE system is a technically complex and unique framework. Consequently, implementing UE in EVE will cover multiple technical fields, such as unique stereoscopic visualisation, creative usage of ambisonic sounding, and high-precision tracking of people. With the help of UE, we plan to control every technical component within one software. As an outcome of our proposal, we will have a more intuitive and easier-to-use software solution.
  <br></div>

  <!-- CONTRIBUTION -->

  <div class="bodyheader">Project's Contribution<br></div>

  <div class="noindent">There was significant damage to the monastery caused by a landslide in 1996. Several buildings and statues got destroyed due to the accident [6]. There are still ongoing renovation processes, and numerous arhat statues are missing. Traces of time appear on the remaining statues’ surfaces. The golden coating is peeling off, and plenty of these statues are in poor condition. For example, some have missing parts. Therefore, our project can function as a 3D archive of cultural heritage which preserves these statues for the future in digital form.<br><br></div>

  <div class="bodytext">As an outcome of our proposal, we plan to create an asset repository and share it with the public on <b>Epic’s Marketplace</b>. Our package will be freely available to anybody. The package will contain 30x high-resolution 3D Arhat statues with accompanying materials.<br><br></div>

  <div class="bodytext">We plan to animate the buddha meshes. The animation will be a continuous morphing animation between two randomly selected meshes. As far as we know, UE cannot natively create morphing motion between unrelated meshes. We found two plugins on Epic’s Marketplace when searching for “morph” keywords. One of them is the Morph Tools Plugin [7] (cost: €49.99), and the second one is the Mesh Morpher [8] (cost: €240.27) which is an Unreal MegaGrants winner project. Both work with a single mesh and morph between various states of that particular mesh and cannot morph a mesh into another one.</div>

  <div class="bodytext">We aim to develop such a morphing solution within Unreal. The outcome will be the project’s second contribution to the Unreal society. We plan to publish this tool on the Marketplace as a <b>free, open-source project</b>.<br><br></div>

  <div class="bodytext">As we mentioned, the EVE system can perform <b>dual-stereoscopic visualisation</b>. The system combines polarised and active stereo technologies to achieve that. Running stereoscopic projections using UE and nDisplay is well documented in the engine’s manual. However, our setup will require a <b>customised configuration</b> for nDisplay. We plan to share the <b>configuration files</b> with a detailed description of achieving dual-stereoscopy with nDisplay.<br><br></div>

  <div class="bodytext">Our long-term goal in implementing UE into the EVE system is to make it more accessible for students. We plan to hire two interns (master level) who will help realise the proposed project. Therefore, they will have to learn Unreal Engine and its accompanying applications (nDisplay, Switchboard, Stage Monitor and RealityCapture). After finishing the proposed project, each intern might remain working as an EVE system administrator. We plan that these students will be able to experiment with their own ideas and help other students realise their projects.</div>

  <div class="bodytext">Since UE’s visual scripting language makes programming accessible to more people, we believe that our system will function as a base for many interdisciplinary collaborations.<br><br></div>

  <div class="bodytext">Once the artwork is working on the EVE platform, we will create a portable version of it, probably on high-end head-worn displays (like Varjo’s XR3) and demo it at high-impact venues, including <b>Real-Time Live at SIGGRAPH</b>.<br><br><br></div>

  <!-- STATEMENT -->

  <div class="bodyheader">Artist Statement<br></div>

  <div class="container">
    <iframe class="responsive-iframe" src="https://www.youtube.com/embed/7CK1HtnUCIE?controls=0&autoplay=1&loop=1&mute=1&playlist=7CK1HtnUCIE" title="10000 Buddhas - Morphing" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>

  <div class="figure">Figure 4. Example for the morphing animation between two randomly selected Arhat statues.<br><br></div>

  <div class="noindent">The &quot;10.000 Buddhas&quot; artwork builds on the tools of the EVE system, except the haptic device. Haptic rendering may be the subject of additional work after completing the MegaGrants, as an extension of the visual and audio experience. The artwork operates with two users. Its main elements are the digital representation of the 500 Arhat statues. In the artwork, each statue is sitting inside a praying wheel. One side of each praying wheel has a hole through which the statues are visible. The praying wheels form a concentric arrangement around each user. These praying wheels are facing outward by default, so the users cannot see the hole on them.<br><br></div>

  <div class="bodytext">The artwork tracks each user&#x27;s attention. When a user focuses on a praying wheel, it rotates and exposes the statue sitting inside. When the users&#x27; focus stays on the same capsule, the statue sitting inside starts to animate. It starts to morph one by one into the other 499 meshes in random order. When the user turns its focus away, the praying wheel turns away, hiding the statue.<br><br><br></div>

  <img src="images/buddha_deity.jpg" loading="lazy" sizes="80vw" srcset="images/buddha_deity-p-500.jpeg 500w, images/buddha_deity-p-800.jpeg 800w, images/buddha_deity-p-1080.jpeg 1080w, images/buddha_deity-p-1600.jpeg 1600w, images/buddha_deity-p-2000.jpeg 2000w, images/buddha_deity-p-2600.jpeg 2600w, images/buddha_deity-p-3200.jpeg 3200w, images/buddha_deity.jpg 3527w" alt="" class="bodyimagefull">

  <div class="figure">Figure 5. Differences between the two virtual worlds. Picture a.) shows the virtual world seen by user 1. Arhat statues are sitting inside the capsules, and the environment shines in golden color. Picture b.) depicts a wrathful deity inside the capsule, and it shines in blueish color. The second user sees this virtual world. <br></div>

  <div class="bodytext">The second user sees a different virtual environment than the first one. This virtual environment shows the same preying wheels, but the statues sitting inside these praying wheels differ. Instead of the Arhat statues, wrathful deities occupy them. Consequently, the artwork includes two virtual environments. They are in contrast with each other, both visually and conceptually.<br></div>

  <div class="bodytext">Example of visual contrast: The Arhat statues&#x27; coating is golden, but the wrathful deities&#x27; have a blueish color. According to color theory, orange and blue are in a complementary relationship, in perfect contrast to each other.<br><br></div>

  <img src="images/multi-user_converted.jpg" loading="lazy" sizes="80vw" srcset="images/multi-user_converted-p-800.jpeg 800w, images/multi-user_converted-p-1080.jpeg 1080w, images/multi-user_converted-p-1600.jpeg 1600w, images/multi-user_converted.jpg 1920w" alt="" class="bodyimagefull">

  <div class="figure">Figure 6. The user on the left sees everything in golden color. For her the Arhant statues populate the capsules. On the other hand, the right user sees everything in blue and instead of the Arhant statues wrathful deities are sitting inside the capsules.<br>‍<br></div>

  <div class="bodytext">The interaction design for both users is the same. When the two users&#x27; attention directions meet, it triggers another behavior of the artwork: it swaps the virtual worlds between the users. From now on, the user seeing the Arhat statues will see the wrathful deities and vice-versa.<br>‍<br></div>

  <img src="images/sound1.jpg" loading="lazy" sizes="80vw" class="bodyimagefull">

  <div class="figure">Figure 7. The installation uses the ambisonic sound system in two different ways. One usage is to emit heavy bass from the subwoofers while the capsules are rotating. It aims to create vibrotactile feedback for the users and alleviate the immersion. The red objects indicate the currently active speakers.<br></div>

  <div class="bodytext">The installation&#x27;s sound design has two roles. First of all, it aims to support the immersion in the artwork. It supports the rotation of the praying wheels and generates vibrotactile feedback [4]. During the rotation of the praying wheels, the sound system emits heavy bass. The aim is to create resonance, which can stimulate the physical senses in the human body, thus creating a more immersive connection between the real and the virtual world.<br><br></div>

  <div class="bodytext">The audio&#x27;s second role is to lead the users&#x27; gaze. The EVE&#x27;s ambisonic sound system can simulate 3D spatial sound, i.e., it is possible to determine and control the location of the sounds in the perceptual space. For example, a high-pitched sound arising behind a praying wheel might attract the users&#x27; attention.<br><br></div>

  <img src="images/sound2.jpg" loading="lazy" sizes="80vw" class="bodyimagefull">

  <div class="figure">Figure 8. The second usage of the ambisonic sound system distributes higher-pitched sounds to different speakers. This aims to drive the users’ attention. Picture a.) depicts a case where the system emits sounds from two different locations. Each user focuses on a different location. Picture b.) depicts a situation when the sound comes from a single location. The users’ attention meets on the same spot.</div>

  <!-- TEAM -->

  <div class="bodyheader">Team<br></div>

  <div class="noindent">The <b>PI</b> has over 20 years of experience in building AR and VR systems. The PI’s expertise in Augmented Reality [10] and Spatial User Interaction [11] is crucial for developing this artwork. He has recently accepted a Professorship at Université Paris-Saclay (UPSaclay), where he is coleading the VENISE laboratory, together with Dr. Patrick Bourdot. Before that, he was the director of the AR Lab at the City University of Hong Kong, School of Creative Media. For the 20 years, the PI has conducted much research using Unreal Engine. The following projects used <b>Unreal Engine</b> concretely. Research on haptic devices and co-located user experience [12], AR exposure and biofeedback [13, 14], and immersive stereoscopic visualisation [15]. Having practised in colocated user experience, haptic devices, and immersive stereoscopic visualisation will serve as solid ground in the development of our proposed project. <br></div>

  <div class="bodytext"><br>The PI will work closely together with a <b>Research Engineer</b>, who will take care of the implementation of this project. Currently, the EVE room is running on Unity combined with MiddleVR. Replacing this pipeline with UE will be one of the first steps in this project; due to the highly specialized rendering setup, we estimate several months of work for this task. The research engineer will be supported by several <b>internship students</b> (master’s level), who will take care of tasks, for example, cleaning up mesh data, learning UE and its implementation to the EVE system, and operating the EVE system using UE.</div>

  <div class="bodytext"><br>The <b>VENISE team</b> has worked on the EVE system since 2010. Patrick Bourdot has studied multimodal rendering in virtual environments since 1998 [16]. This background research will contribute strongly to the project since it combines image, sound, and vibrotactile feedback. Additionally, his research on hand-free navigation in Virtual Reality systems will help implement head-tracking into the artwork [17]. Further members of the VENISE team can contribute to this project: Weiya Chen's and Nicolas Lavedeze's research on multi-stereoscopy [18] contributes to the multi-stereoscopic rendering techniques. Regarding audio in immersive systems, Tifanie Bouchara's experience [19] can support the project strongly.</div>

  <div class="bodytext">Once the artwork is running on the EVE platform, we will create a portable version of it, probably on high-end head-worn displays (like Varjo’s XR3) so that we can demo it at high-impact venues, including Real-Time Live at SIGGRAPH. <br><br><br></div>

  <!-- CONCLUSION -->

  <div class="bodyheader">Conclusion<br></div>

  <div class="noindent">10.000 Buddhas is an art project developed in a technically complex framework. This framework is the EVE system engineered by the VENISE research group. The project aims to implement UE into this framework. As described above, the expertise of the PI and the research team ensures relevant guidance throughout the work in progress.<br><br></div>

  <div class="bodytext">The project's main contributions will be an <b>asset set of high-quality</b> Arhat <b>statues</b>, a <b>morphing animation tool</b> for creating morphs between unrelated 3D meshes, and <b>nDisplay configuration files</b> for configuring dual-stereoscopic visualization. Moreover, <b>tutorials</b> will accompany the project files, accessible through the artwork's dedicated website [20].<br><br></div>

  <div class="bodytext">Changing <b>from Unity to UE</b>, our long-term goal is to control every technical apparatus of the EVE system within UE. By focusing the control on one powerful software, we aim to make it accessible for more students. We believe having interdisciplinary teams of artists, engineers, and scientists will induce the proliferation of high-quality content.<br>‍<br><br></div>

  <div class="bodytext">Additionally, exposing the artwork to the public via <b>conferences and exhibitions</b> is a vital part of the undertaking. Exhibitions might be good events to showcase UE's manifold possibilities in controlling technically-complex, real-time virtual environments.<br><br><br><br></div>

  <!-- REFERENCES -->

  <div class="bodyheader">References<br>‍<br></div>

  <div class="references-text">
    [1] Hong Kong Extras. Ten Thousand Buddhas Monastery. 2010. Last accessed on: 12 November 2021. <a href="http://www.hongkongextras.com/_ten_thousand_buddhas_monastery.html">http://www.hongkongextras.com/_ten_thousand_buddhas_monastery.html</a><br>
    [2] Digiscope. EVE. 2015. Last accessed on: 12 November 2021. <a href="http://digiscope.fr/en/platforms/eve">http://digiscope.fr/en/platforms/eve</a><br>
    [3] Britannica, The Editors of Encyclopaedia. arhat. 2005. Last accessed on: 12 November 2021. <a href="https://www.britannica.com/topic/arhat">https://www.britannica.com/topic/arhat</a><br>
    [4] Advanced Realtime Tracking GmbH & Co. KG. UnrealDTrackPlugin. 2020. Version 0.73. Last accessed on: 10 May 2022. <a href="https://github.com/ar-tracking/UnrealDTrackPlugin">https://github.com/ar-tracking/UnrealDTrackPlugin</a><br>
    [5] ValveSoftware. Steam Audio. 2021. Version 4.0.3. Last accessed on: 10 May 2022. <a href="https://github.com/ValveSoftware/steam-audio/releases">https://github.com/ValveSoftware/steam-audio/releases</a><br>
    [6] N. Lee. Mudslide buries house at Ten Thousand Buddhas Monastery, caretaker feared dead. 1997. Last accessed on: 10 May 2022. <a href="https://www.scmp.com/article/202504/mudslide-burieshouseten- thousand-buddhas-monastery-caretaker-feared-dead">https://www.scmp.com/article/202504/mudslide-burieshouseten- thousand-buddhas-monastery-caretaker-feared-dead</a><br>
    [7] kostenickj. Morph Tool Plugins. 2019. Last accessed on: 10 May 2022. <a href="https:// www.unrealengine.com/marketplace/en-US/product/morph-tools-plugin">https:// www.unrealengine.com/marketplace/en-US/product/morph-tools-plugin</a><br>
    [8] Pug Life Studio. Mesh Morpher. 2020. Version 2.0.3. Last accessed on: 10 May 2022. <a href="https://www.unrealengine.com/marketplace/en-US/product/mesh-morpher">https://www.unrealengine.com/marketplace/en-US/product/mesh-morpher</a><br>
    [9] M. Lee, G. Bruder, T. Höllerer and G. Welch. Effects of Unaugmented Periphery and Vibrotactile Feedback on Proxemics with Virtual Humans in AR. In <i>IEEE Transactions on Visualization and Computer Graphics</i>. vol. 24, no. 4, pp. 1525-1534. 2018.<br>
    [10] C. Sandor. My Past And Upcoming 22 Years With Augmented Reality. <i>Informatique et sciences numériques (chaire annuelle 2021-2022)</i>. 2022.<br>
    [11] C. Sandor, K. Johnsen, S. Serafin (eds.). In <i>Proceedings of the ACM Symposium on Spatial User Interaction</i>. ACM, 2018.<br>
    [12] U. Eck, L. Hoang, C. Sandor, G. Yamamoto, T. Taketomi, H. Kato, H. Laga. Exploring the Perception of Co-Location Errors during Tool Interaction in Visuo-Haptic Augmented Reality. Poster in <i>Proceedings of IEEE International Conference on Virtual Reality</i>. 2016.<br>
    [13] D. Eckhoff, A. Cassinelli, C. Sandor. Heat Pain Threshold Modulation By Experiencing Burning Hands in Augmented Reality. To Appear In <i>2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</i>. 2021<br>
    [14] D. Eckhof, C. Li, G. Cheing, A. Cassinelli, C. Sandor. Investigation of Microcirculatory Effects of Experiencing Burning Hands in Augmented Reality. <i>2021 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</i>. 2021<br>
    [15] A. Rodriguez, A. Cassinelli, C. Sandor. HORIZON. AR-Lab.com. Last accessed on: 23 May 2022. <a href="https://ar-lab.org/project/horizon/">https://ar-lab.org/project/horizon/</a><br>
    [16] G. Bouyer, P. Bourdot, and M. Ammi. Supervision of task-oriented multimodal rendering for VR applications. In <i>Proceedings of the 13th Eurographics conference on Virtual Environments (EGVE’07)</i>. Eurographics Association, Goslar, DEU, pp.93–100. 2007.<br>
    [17] P. Bourdot, M. Dromigny, L. Arnal. Virtual Navigation Fully Controlled by Head Tracking. In <i>Proc. of International Scientific Workshop on Virtual Reality and Prototyping, Laval (France)</i>. 1999.<br>
    [18] W. Chen, N. Ladeveze, C. Clavel, D. Mestre and P. Bourdot. User cohabitation in multistereoscopic immersive virtual environment for individual navigation tasks. <i>2015 IEEE Virtual Reality (VR)</i>. pp.47-54. 2015.<br>
    [19] B. Caramiaux, S. F. Alaoui, T. Bouchara, G. Parseihian, M. Rébillat. Gestural auditory and visual interactive platform. In <i>14th International Conference on Digital Audio Effects (DAFx-11)</i>. Paris, France, pp.167-170. 2014.<br> 
    [20] 10000Buddhas. Submission for the Epic MegaGrants Scheme. 2021. Last accessed on: 21 May 2022. <a href="https://10kbuddhas.github.io/10000buddhas/">https://10kbuddhas.github.io/10000buddhas/</a><br>
  </div>

  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=6183c51c4606b562045fbe78" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>
