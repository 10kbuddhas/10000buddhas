<!DOCTYPE html><!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Sun Nov 07 2021 09:43:13 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="6183c51c4606b5dd8c5fbe79" data-wf-site="6183c51c4606b562045fbe78">
<head>
  <meta charset="utf-8">
  <title>10000buddhas</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/10000buddhas.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Exo:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
</head>
<body class="body">
  <div class="headerimage w-clearfix">
    <div class="textbg w-clearfix">
      <h1 class="heading-3">10.000 Buddhas</h1>
      <div class="text-block-left">Principal Investigator: <b>Christian Sandor</b><br>Host Institution: <b>VENISE TEAM /<br>Paris-Saclay University</b></div>
      <div class="text-block-2">Submission for the:</div>
      <div class="text-block-2-copy">Unreal MegaGrants Scheme</div>
    </div>
  </div>
  <div class="bodyheader-copy">Abstract<br></div>
  <div class="bodytext">10.000 Buddhas is a digital art installation using VR immersive environments. The artwork&#x27;s central elements are the statues at the <a href="http://www.hongkongextras.com/_ten_thousand_buddhas_monastery.html" target="_blank">Ten Thousand Buddhas Monastery</a> in Hong Kong [1]. Digital replicas of these statues populate the virtual environment. Using the <a href="http://digiscope.fr/en/platforms/eve" target="_blank">EVE (Evolutive Virtual Environment)</a> system [2], the users can immerse themselves in the monastery&#x27;s virtual reinterpretation.<br></div>
  <div class="bodytext">The EVE system is a multi-user CAVE allowing double stereoscopy. It incorporates high-quality hardware, such as an ambisonic sound system, high-precision tracking, and haptic devices. This project will utilize the Unreal Engine to control the listed technology and provide a holistic experience.<br></div>
  <div class="bodytext">This project aims to share the emerging aesthetic and technological findings with the Unreal Community.<br></div><img src="images/overview_converted.jpg" loading="lazy" sizes="80vw" srcset="images/overview_converted-p-500.jpeg 500w, images/overview_converted-p-800.jpeg 800w, images/overview_converted-p-1600.jpeg 1600w, images/overview_converted.jpg 1920w" alt="" class="bodyimagefull">
  <p class="figure">Figure 1. Overview of the 10.000 Buddhas artwork during operation in the EVE system. Due to the system&#x27;s double stereoscopy feature, it is possible to visualize different environments for two users.<br></p>
  <div class="bodyheader">Description<br></div>
  <div class="bodytext">This section introduces the project&#x27;s conceptual and technical framework. It describes how the project benefits from Unreal Engine&#x27;s vast features. Moreover, this section mentions how the Unreal Community can benefit from the project.<br></div>
  <div class="bodyheader2"><br><br><br>EVE System</div>
  <div class="bodytext">The EVE system is a unique, multi-user CAVE that provides multi-user 3D perception. The system includes three 5m high HD screens, which enclose a 13 m<sup>2</sup> floor-glass screen. The system provides 3D tracking of people, high-end 3D audio, and a large-scale haptic device.<br><br></div><img src="images/eve_system.png" loading="lazy" sizes="80vw" srcset="images/eve_system-p-500.png 500w, images/eve_system-p-800.png 800w, images/eve_system-p-1080.png 1080w, images/eve_system-p-1600.png 1600w, images/eve_system.png 1920w" alt="" class="bodyimagefull">
  <p class="figure">Figure 2. Documentation of the EVE system in-situ. The system features four projection screens with the possibility of stereoscopic visualization, high-precision trackers, haptic devices, and an ambisonic sound system.<br></p>
  <div class="bodyheader2">Unreal Engine</div>
  <div class="bodytext">The project aims to use Unreal Engine as extensively as possible. Therefore, this artwork will rely on the game engine and additional applications included with the Engine (Switchboard, nDisplay) and software part of the Epic Games (RealityCapture). Ideally, the project would use only Unreal Engine and its components to control every aspect of the EVE system.<br><br></div><img src="images/2_photogrammetry.jpg" sizes="80vw" srcset="images/2_photogrammetry-p-500.jpeg 500w, images/2_photogrammetry-p-800.jpeg 800w, images/2_photogrammetry.jpg 1917w" alt="" class="bodyimagefull">
  <p class="figure">Figure 3. Digitalization of the Arhat statues. Picture a.) shows the real statue at the 10.000 Buddhas Monastery. Picture b.) shows the result of the photogrammetry process.<br></p>
  <div class="bodytext">Digitalization of the <a href="https://www.britannica.com/topic/arhat" target="_blank">Arhat</a> [3] statues happens using photogrammetry. The Arhat statues are the various sculptures residing on the path leading to the Ten Thousand Buddhas Monastery. The software of choice for this task is Reality Capture since it provides fast and high-quality service.<br><br></div>
  <div class="bodytext">Unreal Engine&#x27;s visual scripting language is very appealing for artists, especially those unfamiliar with scripting but who want to develop interactive artworks. In this case, this node-based scripting system covers the game level design, adding interactive properties to the game elements and processing the tracking sensors&#x27; data.<br>‍<br></div>
  <div class="bodytext">The EVE system is a complex framework, which includes projection, audio, and tracking. It is, therefore, necessary to control many instances of Unreal on remote computers with ease. To handle the communication between many devices, Switchboard is the ideal application.<br>‍<br></div>
  <div class="bodytext">The EVE system has four projection screens. Moreover, it provides stereoscopic 3D visualization for two users, meaning that the software of choice must handle four synchronized video outputs. The nDisplay technology is a perfect choice for this role. However, achieving dual-stereoscopic projection in the context of the EVE system (combining polarized and active stereo technologies) is not a straightforward task. The project aims to develop a framework for this technology within Unreal. This framework will be accessible to the Unreal Community as an open-source project file.<br></div>
  <div class="bodytext">For handling spatial audio rendering in the EVE environment, MetaSounds can be a perfect solution. The Engine&#x27;s node-based programming system for audio is a relevant choice for artists. However, solving ambisonics might require combining MetaSounds and WWise: MetaSound will generate the sound, and WWise will distribute it to the ambisonic system.<br><br></div>
  <div class="bodytext">The proposed artwork will be the first to integrate Unreal Engine into the EVE system. Currently, the EVE system works with Unity in combination with MiddleVR and MAX/MSP or PureData is responsible for controlling the 3D sound system.<br><br></div>
  <div class="bodytext">In conclusion, the EVE system is a technically complex and unique framework. Consequently, implementing Unreal Engine for rendering in EVE will cover multiple technical fields, such as unique stereoscopic visualization, creative usage of ambisonic sounding, and high-precision tracking of people. The emerging knowledge during the development of the project will be documented and published. The publication would include open-source project files, presentations, tutorials, and publications at various conferences and exhibitions.<br><br><br>‍<br></div>
  <div class="bodyheader">Artist Statement<br></div>
  <div class="vimeo">
    <iframe width="100%" height="1080" src="https://www.youtube.com/embed/7CK1HtnUCIE?controls=0&autoplay=1&loop=1&mute=1&playlist=7CK1HtnUCIE" title="10000 Buddhas - Morphing" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>
  <div class="figure">Figure 4. Example for the morphing animation between two randomly selected Arhat statues.<br><br></div>
  <div class="bodytext">The &quot;10.000 Buddhas&quot; artwork builds on the tools of the EVE system, except the haptic device. Haptic rendering may be the subject of additional work after completing the MegaGrants, as an extension of the visual and audio experience. The artwork operates with two users. Its main elements are the digital representation of the 500 Arhat statues. In the artwork, each statue is sitting inside a praying wheel. One side of each praying wheel has a hole through which the statues are visible. The praying wheels form a concentric arrangement around each user. These praying wheels are facing outward by default, so the users cannot see the hole on them.<br><br></div>
  <div class="bodytext">The artwork tracks each user&#x27;s attention. When a user focuses on a praying wheel, it rotates and exposes the statue sitting inside. When the users&#x27; focus stays on the same capsule, the statue sitting inside starts to animate. It starts to morph one by one into the other 499 meshes in random order. When the user turns its focus away, the praying wheel turns away, hiding the statue.<br><br><br></div><img src="images/buddha_deity.jpg" loading="lazy" sizes="80vw" srcset="images/buddha_deity-p-500.jpeg 500w, images/buddha_deity-p-800.jpeg 800w, images/buddha_deity-p-1080.jpeg 1080w, images/buddha_deity-p-1600.jpeg 1600w, images/buddha_deity-p-2000.jpeg 2000w, images/buddha_deity-p-2600.jpeg 2600w, images/buddha_deity-p-3200.jpeg 3200w, images/buddha_deity.jpg 3527w" alt="" class="bodyimagefull">
  <div class="figure">Figure 5. Differences between the two virtual worlds. Picture a.) shows the virtual world seen by user 1. Arhat statues are sitting inside the capsules, and the environment shines in golden color. Picture b.) depicts a wrathful deity inside the capsule, and it shines in blueish color. The second user sees this virtual world. <br></div>
  <div class="bodytext">The second user sees a different virtual environment than the first one. This virtual environment shows the same preying wheels, but the statues sitting inside these praying wheels differ. Instead of the Arhat statues, wrathful deities occupy them. Consequently, the artwork includes two virtual environments. They are in contrast with each other, both visually and conceptually.<br></div>
  <div class="bodytext">Example of visual contrast: The Arhat statues&#x27; coating is golden, but the wrathful deities&#x27; have a blueish color. According to color theory, orange and blue are in a complementary relationship, in perfect contrast to each other.<br><br></div><img src="images/multi-user_converted.jpg" loading="lazy" sizes="80vw" srcset="images/multi-user_converted-p-800.jpeg 800w, images/multi-user_converted-p-1080.jpeg 1080w, images/multi-user_converted-p-1600.jpeg 1600w, images/multi-user_converted.jpg 1920w" alt="" class="bodyimagefull">
  <div class="figure">Figure 6. The user on the left sees everything in golden color. For her the Arhant statues populate the capsules. On the other hand, the right user sees everything in blue and instead of the Arhant statues wrathful deities are sitting inside the capsules.<br>‍<br></div>
  <div class="bodytext">The interaction design for both users is the same. When the two users&#x27; attention directions meet, it triggers another behavior of the artwork: it swaps the virtual worlds between the users. From now on, the user seeing the Arhat statues will see the wrathful deities and vice-versa.<br>‍<br></div><img src="images/sound.jpg" loading="lazy" sizes="80vw" srcset="images/sound-p-500.jpeg 500w, images/sound-p-800.jpeg 800w, images/sound-p-1600.jpeg 1600w, images/sound.jpg 1917w" alt="" class="bodyimagefull">
  <div class="figure">Figure 7. The installation uses the ambisonic sound system in two different ways. Picture a.) depicts the speakers emitting heavy bass to produce vibrotactile feedback for the users. This situation happens while a capsule is rotating. Picture b.) depicts how the system will distribute the sound to the different speakers to drive the users&#x27; attention. In both pictures, the red objects indicate the currently working speakers. <br></div>
  <div class="bodytext">The installation&#x27;s sound design has two roles. First of all, it aims to support the immersion in the artwork. It supports the rotation of the praying wheels and generates vibrotactile feedback [4]. During the rotation of the praying wheels, the sound system emits heavy bass. The aim is to create resonance, which can stimulate the physical senses in the human body, thus creating a more immersive connection between the real and the virtual world.<br><br></div>
  <div class="bodytext">The audio&#x27;s second role is to lead the users&#x27; gaze. The EVE&#x27;s ambisonic sound system can simulate 3D spatial sound, i.e., it is possible to determine and control the location of the sounds in the perceptual space. For example, a high-pitched sound arising behind a praying wheel might attract the users&#x27; attention.<br><br></div>
  <div class="bodytext">In addition to technical benefits, this project is suitable to communicate higher-level achievements using Unreal Engine. For example, as an artwork, the project can showcase the rendering aesthetics of the software. It can also introduce new and creative ideas for using ambisonic sounding and interaction design in artistic practices.<br><br>‍<br><br></div>
  <div class="bodyheader">Team and Motivation<br><br></div>
  <div class="bodyheader">Conclusion<br></div>
  <div class="bodytext">10.000 Buddhas is an art project developed in a technically complex framework. This framework is the EVE system engineered by the VENISE research group. The project aims to implement Unreal Engine into this framework and solve as many system aspects as possible in this Engine. As described above, the expertise of my supervisor and the research team ensures relevant guidance throughout the work in progress.<br><br></div>
  <div class="bodytext">The development process of the artwork will be regularly documented and published as open-source project files. Moreover, tutorials will accompany the project files, accessible through the artwork&#x27;s <a href="http://www.10000buddhas.art" target="_blank">dedicated website</a> [12].<br><br></div>
  <div class="bodytext">Additionally, exposing the artwork to the public via conferences and exhibitions is a vital part of the undertaking. Exhibitions might be good events to showcase Unreal&#x27;s aesthetic benefits in real-time rendering.<br>‍<br><br></div>
  <div class="bodyheader">References<br>‍<br></div>
  <div class="text-block-5">[1] http://www.hongkongextras.com/_ten_thousand_buddhas_monastery.html<br>[2] http://digiscope.fr/en/platforms/eve<br>[3] https://www.britannica.com/topic/arhat<br>[4]  M. Lee, G. Bruder, T. Höllerer and G. Welch. 2018. Effects of Unaugmented Periphery and Vibrotactile Feedback on Proxemics with Virtual Humans in AR. In IEEE Transactions on Visualization and Computer Graphics. vol. 24, no. 4, pp. 1525-1534<br>[5] https://www.ar-lab.org<br>[6] https://vr-in-industry.de/programm/sprecher/sandor.html<br>[7] 2016. Proceedings of the 2016 Symposium on Spatial User Interaction. Association for Computing Machinery, New York, NY, USA.<br>[8] G. Bouyer, P. Bourdot, and M. Ammi. 2007. Supervision of task-oriented multimodal rendering for VR applications. In Proceedings of the 13th Eurographics conference on Virtual Environments (EGVE&#x27;07). Eurographics Association, Goslar, DEU, pp.93–100.<br>[9] Bourdot, Patrick &amp; Dromigny, Martin &amp; Arnal, Laurent. 1999. Virtual Navigation Fully Controlled by Head Tracking. <br>[10] W. Chen, N. Ladeveze, C. Clavel, D. Mestre and P. Bourdot. 2015. User cohabitation in multi-stereoscopic immersive virtual environment for individual navigation tasks. 2015 IEEE Virtual Reality (VR). pp.47-54.<br>[11] Baptiste Caramiaux, Sarah Fdili Alaoui, Tifanie Bouchara, Gaétan Parseihian, Marc Rébillat. 2014. Gestural auditory and visual interactive platform. In 14th International Conference on Digital Audio Effects (DAFx-11). Paris, France, pp.69.<br>[12] www.10000buddhas.art</div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=6183c51c4606b562045fbe78" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>
